\section{Usage and challenges}

We expect that searches for non-resonant $\dihiggs$ production in the channel $\Pbottom\Pbottom\PW\PW^{*}$ at the LHC
will benefit from using a multivariate approach, for example based on a BDT or Neural Network (NN)~\cite{TensorFlow,Keras},
using the likelihood ratio $P$ given by Eq.~(\ref{eq:memLR}) and the marginalized one $P_{\textrm{m}}$ as input,
together with other observables such as the number of $\Pbottom$-jet candidates and their $\Pbottom$-tagging discriminant values, 
the mass of the two jets with the highest $\Pbottom$-tagging discriminant, and other kinematic observables.
Other, subdominant, backgrounds can be included in the training of the BDT or NN
to further improve the separation of the $\dihiggs$ signal from backgrounds.

At the point this paper is finished there was an active field the computations of the $\dihiggs$ process at NLO in QCD (see for example~\cite{deFlorian:2013jea, Borowka:2016ehy, Heinrich:2017kxx}). 
One of the main effects of the correction on the computation is a sizeable shifting on the $\dihiggs$ system $p_T$ while,  on the other hand,
the most important signal feature for the signal discrimination ($m_{\dihiggs}$)  remains mostly resilient against NLO effects. 
In the definition of the MEM algorithm we had made the effort of make the MEM be mostly dependent of radiation resilient variables when we decided to calculate it on the ZTM frame. 

One remaining issue in practical applications of the MEM may be the computing time requirements.
Experimental analyses will usually need to evaluate the integrals in Eqs.~(\ref{eq:mem_signal}) and~(\ref{eq:mem_background})
multiple times for each event (and eventually benchmark scenario) 
in order to assess the effect of systematic uncertainties.
Taken together with the large cross section for $\ttbar$ production at the LHC,
the integrals in Eqs.~(\ref{eq:mem_signal}) and~(\ref{eq:mem_background}) will need to be computed in the order of $100$ million times.
Even with several thousands of computing jobs running in parallel,
as it is nowadays commonplace for experimental analyses performed at the LHC,
the computation still requires a few weeks of nonstop computing time.
Several possibilities to speed up the numeric integrations, which take most of the computing time in practical applications of the MEM,
have been explored in the literature.
One alternative is to use vector integrands to evaluate the likelihood ratio for all systematic uncertainties simultaneously~\cite{CUBA},
taking advantage of the fact that the systematic uncertainties typically constitute small changes with respect to the nominal value.
Another alternative is to take advantage of the parallelizability of multidimensional integration and perform the integration on graphics processing units (GPUs).
Speedup factors of order $100$, compared to using a single core of a general-purpose central processing unit (CPU) 
such as the $2.30$~GHz Intel\TReg~Xeon\TReg~E5-2695V3 processor that we used for the studies presented in this paper,
are reported in the literature for performing numeric integrations on GPUs~\cite{Hagiwara:2009aq,Hagiwara:2009cy,Kanzaki:2010ym,Hagiwara:2013oka,Schouten:2014yza,Grasseau:2015vfa}.

